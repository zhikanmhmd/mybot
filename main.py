import os
import requests
from bs4 import BeautifulSoup
from zoneinfo import ZoneInfo
from datetime import datetime
import time
import re
from urllib.parse import urljoin
from requests.exceptions import RequestException
from fake_useragent import UserAgent

from telegram import (
    Update,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
    ReplyKeyboardMarkup,
    KeyboardButton,
)
from telegram.ext import (
    Application,
    CommandHandler,
    CallbackQueryHandler,
    MessageHandler,
    filters,
    ContextTypes,
)
from groq import Groq

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
ALANCHAND_TOKEN = "9jkzPPBrUV6NpVYxPHO4"  # Ø§ÛŒÙ† ØªÙˆÚ©Ù† Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡ØŒ Ø§Ù…Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø´Ø¯Ù‡

GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
GROQ_CLIENT = Groq(api_key=GROQ_API_KEY)

ua = UserAgent()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ/Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ Ù„Ø§ØªÛŒÙ† (Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù† Ø´Ø¯Ù† ÙÙˆÙ†Øª Ø¯Ø± ØªÙ„Ú¯Ø±Ø§Ù…)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def to_latin_digits(text: str) -> str:
    if not text or not text.strip().isdigit():
        return text
    persian_to_latin = str.maketrans('Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹', '0123456789')
    return text.translate(persian_to_latin)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ØªÙˆØ§Ø¨Ø¹ Ù‚ÛŒÙ…Øª Ø¯Ù„Ø§Ø± Ùˆ ØªØªØ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_dollar_alanchand() -> str:
    url = "https://alanchand.com/"
    try:
        headers = {"User-Agent": ua.random}
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code != 200:
            return "Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯"
        soup = BeautifulSoup(res.text, "html.parser")
        for table in soup.find_all("table"):
            for row in table.find_all("tr"):
                cells = row.find_all("td")
                if len(cells) >= 3 and "Ø¯Ù„Ø§Ø± Ø¢Ù…Ø±ÛŒÚ©Ø§" in cells[0].text:
                    sell_price = cells[2].text.strip().replace(",", "")
                    return to_latin_digits(sell_price)
        return "Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯"
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾ÛŒÙ†Ú¯ Ø¯Ù„Ø§Ø±: {e}")
        return "Ø®Ø·Ø§"


def get_tether_nobitex() -> str:
    url = "https://nobitex.ir/price/usdt"
    
    try:
        headers = {
            "User-Agent": ua.random or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        res = requests.get(url, headers=headers, timeout=12)
        if res.status_code != 200:
            return f"Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯ (Ú©Ø¯: {res.status_code})"
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Û±: Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ù‚ÛŒÙ…Øª Ø±Ùˆ Ø¯Ø§Ø±Ù†
        keywords = ["Ù‚ÛŒÙ…Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ", "Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ", "Ø¢Ø®Ø±ÛŒÙ† Ù‚ÛŒÙ…Øª", "Ù‚ÛŒÙ…Øª ØªÙˆÙ…Ø§Ù†ÛŒ", "ØªØªØ±", "USDT"]
        for elem in soup.find_all(['span', 'div', 'p', 'strong', 'h2', 'td']):
            text = elem.get_text(strip=True)
            if any(kw in text for kw in keywords):
                match = re.search(r'(\d{1,3}(?:[,\sÙ¬]\d{3})*)\s*(?:IRT|ØªÙˆÙ…Ø§Ù†| ØªÙˆÙ…Ø§Ù†|$|IRT)', text)
                if match:
                    price_str = match.group(1).replace(',', '').replace('Ù¬', '').replace(' ', '')
                    if len(price_str) in (6, 7):
                        return to_latin_digits(price_str)
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Û²: Ø§Ø¹Ø¯Ø§Ø¯ Û¶-Û· Ø±Ù‚Ù…ÛŒ Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†Ø·Ù‚ÛŒ
        all_text = soup.get_text(separator=" ", strip=True)
        matches = re.findall(r'\b(\d{6,7})\b', all_text)
        if matches:
            for m in matches:
                if 140000 <= int(m) <= 200000:
                    return to_latin_digits(m)
        
        return "Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯ (Ù‚ÛŒÙ…Øª Ù…Ù†Ø§Ø³Ø¨ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯)"
    
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù†ÙˆØ¨ÛŒØªÚ©Ø³: {str(e)[:150]}")
        return "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„"


def get_prices() -> tuple:
    dollar = get_dollar_alanchand()
    tether = get_tether_nobitex()
    return dollar, tether


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ØªÙˆØ§Ø¨Ø¹ Ø¯ÛŒÚ¯Ø± (Ø¹Ú©Ø³ØŒ Ø®Ù„Ø§ØµÙ‡ØŒ ØªÛŒØªØ±Ù‡Ø§)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def escape_markdown_v2(text: str) -> str:
    if not text:
        return ""
    reserved = r'\_*[]()~`>#+-=|{}.!'
    for char in reserved:
        text = text.replace(char, '\\' + char)
    return text


def get_article_image(url: str) -> str | None:
    try:
        headers = {"User-Agent": ua.random or "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            src = og["content"].strip()
            if src.startswith("//"): src = "https:" + src
            if src.startswith("/"): src = urljoin(url, src)
            if any(ext in src.lower() for ext in [".jpg",".jpeg",".png",".webp",".gif"]):
                return src

        twitter = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter and twitter.get("content"):
            src = twitter["content"].strip()
            if src.startswith("//"): src = "https:" + src
            if src.startswith("/"): src = urljoin(url, src)
            if any(ext in src.lower() for ext in [".jpg",".jpeg",".png",".webp"]):
                return src

        for sel in [
            "img.size-full", "img.aligncenter", ".featured-image", ".post-thumbnail",
            ".entry-content img", ".content img", "figure img", "article img"
        ]:
            img = soup.select_one(sel)
            if img:
                for attr in ["src", "data-src", "data-lazy-src", "data-original"]:
                    src = img.get(attr)
                    if src:
                        if src.startswith("//"): src = "https:" + src
                        if src.startswith("/"): src = urljoin(url, src)
                        if any(ext in src.lower() for ext in [".jpg",".jpeg",".png",".webp"]) and "logo" not in src and "avatar" not in src:
                            return src

        return None

    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¹Ú©Ø³ {url}: {str(e)[:100]}")
        return None


async def generate_summary(url: str) -> str:
    if not url or not url.startswith(('http://', 'https://')):
        return "Ù„ÛŒÙ†Ú© Ù†Ø§Ù…Ø¹ØªØ¨Ø±"

    try:
        headers = {"User-Agent": ua.random or "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return f"Ø®Ø·Ø§ {response.status_code}"

        soup = BeautifulSoup(response.text, "html.parser")
        content = ""
        for cls in ['content','post-content','entry-content','article-body','news-content','body']:
            div = soup.find(['div','article'], class_=cls)
            if div:
                content = div.get_text(separator="\n", strip=True)
                break

        if len(content) < 200:
            ps = [p.get_text(strip=True) for p in soup.find_all('p') if len(p.get_text(strip=True)) > 30]
            content = "\n".join(ps)

        if len(content) < 100:
            return "Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯"

        content = content[:7000]

        completion = GROQ_CLIENT.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": "ÙÙ‚Ø· ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø¯Ù‚ÛŒÙ‚ (Û± ØªØ§ Û² Ø¬Ù…Ù„Ù‡) Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³. Ø±Ø³Ù…ÛŒ Ùˆ Ø±ÙˆØ§Ù†. Ø§Ú¯Ø± Ù…ØªÙ† Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ Ø¨Ù†ÙˆÛŒØ³: Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯."},
                {"role": "user", "content": content}
            ],
            max_tokens=120,
            temperature=0.35,
        )

        summary_text = completion.choices[0].message.content.strip()
        return summary_text if summary_text and len(summary_text) >= 10 else "Ø®Ù„Ø§ØµÙ‡ ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯"

    except Exception as ex:
        print(f"Ø®Ø·Ø§ summarize {url}: {str(ex)[:150]}")
        return "Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"


def get_economic_headlines(limit=5):
    url = "https://zoomon.ir/"
    headers = {"User-Agent": ua.random or "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=12)
        soup = BeautifulSoup(response.text, "html.parser")
        headlines = []
        for a in soup.find_all('a', href=True):
            text = a.get_text(strip=True)
            if len(text) > 20 and 'Ù…Ø·Ø§Ù„Ø¹Ù‡' not in text and 'Ú©Ø§Ù…Ù†Øª' not in text:
                href = a['href']
                if href.startswith('/'): href = "https://zoomon.ir" + href
                if href.startswith('http'):
                    headlines.append((text, href))
                    if len(headlines) >= limit: break
        return headlines[:limit] or [("ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.", None)]
    except Exception as e:
        print(f"Ø®Ø·Ø§ ØªÛŒØªØ± Ø§Ù‚ØªØµØ§Ø¯ÛŒ: {e}")
        return [("Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ ØªÛŒØªØ±Ù‡Ø§", None)]


def get_zoomit_tech_headlines(limit=5):
    url = "https://www.zoomit.ir/"
    headers = {"User-Agent": ua.random or "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=12)
        soup = BeautifulSoup(response.text, "html.parser")
        headlines = []
        for a in soup.find_all('a', href=True):
            text = a.get_text(strip=True)
            if len(text) > 25 and any(kw in text.lower() for kw in ['Ù„Ùˆ Ø±ÙØª','Ù…Ø¹Ø±ÙÛŒ','Ù‡ÙˆØ´','Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯','Ø§Ù¾Ù„','Ú¯ÙˆØ´ÛŒ','Ù„Ù¾','ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ']):
                href = a['href']
                if href.startswith('/'): href = "https://www.zoomit.ir" + href
                if href.startswith('https://www.zoomit.ir/'):
                    headlines.append((text, href))
                    if len(headlines) >= limit: break
        return headlines[:limit] or [("ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.", None)]
    except Exception as e:
        print(f"Ø®Ø·Ø§ ØªÛŒØªØ± Ø²ÙˆÙ…ÛŒØª: {e}")
        return [("Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø²ÙˆÙ…ÛŒØª", None)]


def get_zoomg_cinema_game_headlines(limit=5):
    url = "https://www.zoomg.ir/"
    headers = {"User-Agent": ua.random or "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=12)
        soup = BeautifulSoup(response.text, "html.parser")
        headlines = []
        for a in soup.find_all('a', href=True):
            text = a.get_text(strip=True)
            if len(text) > 25 and any(kw in text.lower() for kw in ['ÙÛŒÙ„Ù…','Ø¨Ø§Ø²ÛŒ','Ø³ÛŒÙ†Ù…Ø§','Ú¯ÛŒÙ…','Ø³Ø±ÛŒØ§Ù„','Ù…Ø§ÛŒÚ©Ø±ÙˆØ³Ø§ÙØª','Ù†Ù‚Ø¯','ØªØ±ÛŒÙ„Ø±']):
                href = a['href']
                if href.startswith('/'): href = "https://www.zoomg.ir" + href
                if href.startswith('https://www.zoomg.ir/'):
                    headlines.append((text, href))
                    if len(headlines) >= limit: break
        return headlines[:limit] or [("ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.", None)]
    except Exception as e:
        print(f"Ø®Ø·Ø§ ØªÛŒØªØ± Ø²ÙˆÙ…Ø¬ÛŒ: {e}")
        return [("Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø²ÙˆÙ…Ø¬ÛŒ", None)]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ú©ÛŒØ¨ÙˆØ±Ø¯ Ø§ØµÙ„ÛŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_main_reply_keyboard():
    keyboard = [
        [KeyboardButton("ğŸ’° Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ Ø¯Ù„Ø§Ø± Ùˆ ØªØªØ±"),     KeyboardButton("â° ØªÙ†Ø¸ÛŒÙ… Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±")],
        [KeyboardButton("ğŸ“° ØªÛŒØªØ±Ù‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø§Ù…Ø±ÙˆØ²"),   KeyboardButton("ğŸ–¥ï¸ Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ")],
        [KeyboardButton("ğŸ®ğŸ¬ Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ú¯ÛŒÙ…"),             KeyboardButton("ğŸ›‘ Ù„ØºÙˆ Ù‡Ù…Ù‡ Ø§Ø¹Ù„Ø§Ù†â€ŒÙ‡Ø§")],
        [KeyboardButton("â„¹ï¸ Ø±Ø§Ù‡Ù†Ù…Ø§"),                    KeyboardButton("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¯ÙˆØ¨Ø§Ø±Ù‡")],
    ]
    return ReplyKeyboardMarkup(
        keyboard=keyboard,
        resize_keyboard=True,
        one_time_keyboard=False,
        is_persistent=True,
        input_field_placeholder="ÛŒÚ©ÛŒ Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†..."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ú©ÛŒØ¨ÙˆØ±Ø¯ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_interval_keyboard():
    return InlineKeyboardMarkup([
        [InlineKeyboardButton("Ù‡Ø± Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡", callback_data="interval_5m")],
        [InlineKeyboardButton("Ù‡Ø± Û± Ø³Ø§Ø¹Øª",   callback_data="interval_1h")],
        [InlineKeyboardButton("Ù‡Ø± Û³ Ø³Ø§Ø¹Øª",   callback_data="interval_3h")],
        [InlineKeyboardButton("Ù‡Ø± Û¶ Ø³Ø§Ø¹Øª",   callback_data="interval_6h")],
        [InlineKeyboardButton("Ø¨Ø§Ø²Ú¯Ø´Øª",      callback_data="back")],
    ])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ø§Ø±Ø³Ø§Ù„ Ù‚ÛŒÙ…Øª Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def send_price(context: ContextTypes.DEFAULT_TYPE):
    dollar, tether = get_prices()
    now = datetime.now(ZoneInfo('Asia/Tehran')).strftime('%H:%M')
    text = f"ğŸª™ Ù‚ÛŒÙ…Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ ({now})\n\nğŸ’° Ø¯Ù„Ø§Ø±: {dollar} ØªÙˆÙ…Ø§Ù†\nğŸ”— ØªØªØ±: {tether} ØªÙˆÙ…Ø§Ù†"
    await context.bot.send_message(chat_id=context.job.chat_id, text=text, reply_markup=get_main_reply_keyboard())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù‡Ù†Ø¯Ù„Ø± Ø§ØµÙ„ÛŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text.strip()
    chat_id = update.effective_chat.id

    async def send_news_section(section_title: str, items_getter, emoji: str):
        items = items_getter(5)
        if not items or all("Ø®Ø·Ø§" in t[0] for t in items):
            await update.message.reply_text("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.", reply_markup=get_main_reply_keyboard())
            return

        output = escape_markdown_v2(f"{emoji} {section_title}:\n\n")
        photo_sent = False

        for i, (title, url) in enumerate(items, 1):
            photo_url = get_article_image(url)
            summary = await generate_summary(url)
            summary_line = "Ø®Ù„Ø§ØµÙ‡: Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨ÙˆØ¯" if any(w in summary.lower() for w in ["Ø®Ø·Ø§","Ù†ÛŒØ³Øª","Ù†Ø´Ø¯"]) else f"Ø®Ù„Ø§ØµÙ‡: {summary}"
            link_line = f"Ù„ÛŒÙ†Ú©: {url}" if url else ""

            if not photo_sent and photo_url:
                caption = f"{i}. {title}\n{summary_line}\n{link_line}"
                try:
                    await context.bot.send_photo(
                        chat_id=chat_id,
                        photo=photo_url,
                        caption=escape_markdown_v2(caption[:900]),
                        parse_mode="MarkdownV2",
                        reply_markup=get_main_reply_keyboard()
                    )
                    photo_sent = True
                    continue
                except Exception as e:
                    print(f"Ø®Ø·Ø§ Ø§Ø±Ø³Ø§Ù„ Ø¹Ú©Ø³ {url}: {str(e)[:100]}")

            output += f"{i}\. {escape_markdown_v2(title)}\n"
            output += escape_markdown_v2(f"   {summary_line}\n")
            if url:
                output += escape_markdown_v2(f"   Ù„ÛŒÙ†Ú©: {url}\n")
            output += "\n"

        await update.message.reply_text(
            output,
            parse_mode="MarkdownV2",
            disable_web_page_preview=True,
            reply_markup=get_main_reply_keyboard()
        )

    # ØªØ´Ø®ÛŒØµ Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
    if any(w in text for w in ["Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ", "Ø¯Ù„Ø§Ø±", "ØªØªØ±", "ğŸ’°"]):
        dollar, tether = get_prices()
        now = datetime.now(ZoneInfo('Asia/Tehran')).strftime('%H:%M')
        msg = f"ğŸª™ Ù‚ÛŒÙ…Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ ({now})\nØ¯Ù„Ø§Ø±: {dollar} ØªÙˆÙ…Ø§Ù†\nØªØªØ±: {tether} ØªÙˆÙ…Ø§Ù†"
        await update.message.reply_text(msg, reply_markup=get_main_reply_keyboard())

    elif any(w in text for w in ["ØªÙ†Ø¸ÛŒÙ… Ø§Ø±Ø³Ø§Ù„", "Ø®ÙˆØ¯Ú©Ø§Ø±", "â°"]):
        await update.message.reply_text("Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†:", reply_markup=get_interval_keyboard())

    elif any(w in text for w in ["Ø§Ù‚ØªØµØ§Ø¯ÛŒ", "ğŸ“°"]):
        await send_news_section("ØªÛŒØªØ±Ù‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø§Ù…Ø±ÙˆØ²", get_economic_headlines, "ğŸ“°")

    elif any(w in text for w in ["ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ", "Ø²ÙˆÙ…ÛŒØª", "ğŸ–¥ï¸"]):
        await send_news_section("Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ", get_zoomit_tech_headlines, "ğŸ–¥ï¸")

    elif any(w in text for w in ["Ø³ÛŒÙ†Ù…Ø§", "Ú¯ÛŒÙ…", "Ø²ÙˆÙ…Ø¬ÛŒ", "ğŸ®", "ğŸ¬"]):
        await send_news_section("Ø§Ø®Ø¨Ø§Ø± Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ú¯ÛŒÙ…", get_zoomg_cinema_game_headlines, "ğŸ®ğŸ¬")

    elif any(w in text for w in ["Ù„ØºÙˆ", "ğŸ›‘"]):
        for job in context.job_queue.get_jobs_by_name(f"price_{chat_id}"):
            job.schedule_removal()
        await update.message.reply_text("âœ“ Ù‡Ù…Ù‡ Ø§Ø¹Ù„Ø§Ù†â€ŒÙ‡Ø§ Ù„ØºÙˆ Ø´Ø¯", reply_markup=get_main_reply_keyboard())

    elif any(w in text for w in ["Ø±Ø§Ù‡Ù†Ù…Ø§", "â„¹ï¸"]):
        await update.message.reply_text(
            "Ø±Ø§Ù‡Ù†Ù…Ø§:\n"
            "ğŸ’° Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ â†’ Ù‚ÛŒÙ…Øª Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ\n"
            "â° ØªÙ†Ø¸ÛŒÙ… Ø§Ø±Ø³Ø§Ù„ â†’ Ù‚ÛŒÙ…Øª Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ\n"
            "ğŸ“° Ø§Ù‚ØªØµØ§Ø¯ÛŒ â†’ Ø§Ø®Ø¨Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÛŒ (Ø¨Ø§ Ø¹Ú©Ø³ Ø®Ø¨Ø± Ø§ÙˆÙ„)\n"
            "ğŸ–¥ï¸ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ â†’ Ø§Ø®Ø¨Ø§Ø± Ø²ÙˆÙ…ÛŒØª (Ø¨Ø§ Ø¹Ú©Ø³ Ø®Ø¨Ø± Ø§ÙˆÙ„)\n"
            "ğŸ®ğŸ¬ Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ú¯ÛŒÙ… â†’ Ø§Ø®Ø¨Ø§Ø± Ø²ÙˆÙ…Ø¬ÛŒ (Ø¨Ø§ Ø¹Ú©Ø³ Ø®Ø¨Ø± Ø§ÙˆÙ„)\n"
            "ğŸ›‘ Ù„ØºÙˆ â†’ Ù‚Ø·Ø¹ Ø§Ø¹Ù„Ø§Ù†â€ŒÙ‡Ø§\n"
            "ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ â†’ Ø±ÛŒØ³Øª Ø¨Ø§Øª\n\n"
            "Ø³ÙˆØ§Ù„ Ø¯Ø§Ø±ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù†ÙˆÛŒØ³!",
            reply_markup=get_main_reply_keyboard()
        )

    elif any(w in text for w in ["Ø´Ø±ÙˆØ¹ Ø¯ÙˆØ¨Ø§Ø±Ù‡", "Ø±ÛŒ Ø§Ø³ØªØ§Ø±Øª", "ğŸ”„"]):
        for job in context.job_queue.get_jobs_by_name(f"price_{chat_id}"):
            job.schedule_removal()
        await update.message.reply_text(
            "Ø¨Ø§Øª Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯! ğŸŒ±\nØ§Ø² Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†:",
            reply_markup=get_main_reply_keyboard()
        )

    else:
        await update.message.reply_text("Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† ğŸ˜Š", reply_markup=get_main_reply_keyboard())


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "Ø³Ù„Ø§Ù…! Ø¨Ù‡ Ø±Ø¨Ø§Øª Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒ ğŸŒŸ\nØ§Ø² Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†:",
        reply_markup=get_main_reply_keyboard()
    )


async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    chat_id = query.from_user.id

    if data == "back":
        await query.edit_message_text("Ø¨Ø§Ø²Ú¯Ø´Øª", reply_markup=None)
        await context.bot.send_message(chat_id, "Ù…Ù†ÙˆÛŒ Ø§ØµÙ„ÛŒ:", reply_markup=get_main_reply_keyboard())
        return

    if data.startswith("interval_"):
        val = data.split("_")[1]
        mapping = {"5m": (300, "Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡"), "1h": (3600, "Û± Ø³Ø§Ø¹Øª"), "3h": (10800, "Û³ Ø³Ø§Ø¹Øª"), "6h": (21600, "Û¶ Ø³Ø§Ø¹Øª")}
        if val not in mapping: return
        sec, disp = mapping[val]

        for job in context.job_queue.get_jobs_by_name(f"price_{chat_id}"):
            job.schedule_removal()

        context.job_queue.run_repeating(
            send_price,
            interval=sec,
            first=15,
            chat_id=chat_id,
            name=f"price_{chat_id}",
        )

        await query.edit_message_text(f"âœ“ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯: Ù‡Ø± {disp}", reply_markup=None)


def main():
    app = Application.builder().token(TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    app.add_handler(CallbackQueryHandler(button_handler))

    print("Ø¨Ø§Øª Ø´Ø±ÙˆØ¹ Ø´Ø¯.")
    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()